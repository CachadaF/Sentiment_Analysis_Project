{
  "cells": [
    {
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%load_ext autoreload\n%autoreload 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8272ef6108d2803125980a8cbed4c5ae0be17c36"
      },
      "cell_type": "markdown",
      "source": "# Models"
    },
    {
      "metadata": {
        "_uuid": "a77de22781f477c899432c0a97324dbbc1b76e9b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input, SimpleRNN\nfrom keras.layers.embeddings import Embedding\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
      },
      "cell_type": "markdown",
      "source": "# Pre-processing"
    },
    {
      "metadata": {
        "_uuid": "dba5fdee6eab073912fb38e690c776f48e0e4884",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport chardet\nimport re\nimport os\nimport random",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Checking files in Kaggle\n# List data files that are connected to the kernel\n\n#os.listdir('../input')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Read Train & Test Files\n\n#Kaggle\ntrain_file = bz2.BZ2File('../input/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n\n#Localhost\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n\n#Localhost - Versión recortada del archivo\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n\n#Create Lists containing Train & Test sentences\ntrain_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()\n\n#Convert from raw binary strings to strings that can be parsed\ntrain_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file, test_file\n#Garbage collector\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\nprint(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
      },
      "cell_type": "markdown",
      "source": "## Clean data"
    },
    {
      "metadata": {
        "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n\n# Make everything Lower Case\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n\n# Modify URLs to <url>\nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
      },
      "cell_type": "markdown",
      "source": "## Checking data before and after cleaning"
    },
    {
      "metadata": {
        "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Random\nr = random.randint(1,len(train_file_lines))\n\n#Before\nprint(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n\n#After\nprint(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n\n#Labels\nprint(\"\\nLabel:{}\".format(train_labels[r-1:r]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
      },
      "cell_type": "markdown",
      "source": "### Output\nFrom the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n\nSo, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
    },
    {
      "metadata": {
        "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file_lines, test_file_lines\n#Garbage collector\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
      },
      "cell_type": "markdown",
      "source": "## Text Pre-processing"
    },
    {
      "metadata": {
        "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer\n\n#Base definitions for text preprocessing\nmax_features = 20000\nmaxlen = 100",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n\n\n#Faltaría hacer lo siguiente\n#1) Eliminar las palabras con longitud mayor a 100 como en el otro.\n#2) Aplicar el filtro de caracteres especiales.\n#3) Chequear como utilizar el Test Set luego.\n#4) Apicarlo\n\n\nv = CountVectorizer(analyzer = \"word\",max_features = max_features)\n\nX_train = v.fit_transform(train_sentences)\nX_test = v.transform(test_sentences)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5381cf4aa759c3cde8c1a3bc775c71eaa0debfae"
      },
      "cell_type": "markdown",
      "source": "### Validation dataset"
    },
    {
      "metadata": {
        "_uuid": "034188709597cb9a4390840f8b4010b8e66a5b13",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n# Create a validation dataset\nvalidation_size = 0.2\nX_train, X_valid, train_labels, valid_labels = train_test_split(X_train, train_labels, test_size = validation_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9c36b66bfc3f7cfe53c50505956e443b643c319",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\n#del tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences\n#Garbage collector\n#gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
      },
      "cell_type": "markdown",
      "source": "## Model"
    },
    {
      "metadata": {
        "_uuid": "9ba4e755b369728f6b3ec6007c5bbb117ba098f9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\n\n#Armo los distintos clasificadores que voy a utilizar -> Revisar posibles hiperparametros como lo hice en NN.\nClassifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    GaussianNB(),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    #DecisionTreeClassifier(),\n    #RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    #KNeighborsClassifier(3),\n    ]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd2138fd181f6d8302bc5d5697453bbdc8ff3f98"
      },
      "cell_type": "markdown",
      "source": "## Test "
    },
    {
      "metadata": {
        "_uuid": "64ea1632ab83398264b37480e87db3e4f5b60a34",
        "trusted": true
      },
      "cell_type": "code",
      "source": "Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    fit = classifier.fit(X_train,train_labels)\n    pred = fit.predict(X_valid)\n    accuracy = accuracy_score(pred,valid_labels)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1e6c3d5a86ee18206ac22f67ad2641962269ac6"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
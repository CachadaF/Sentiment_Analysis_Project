{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8272ef6108d2803125980a8cbed4c5ae0be17c36"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "a77de22781f477c899432c0a97324dbbc1b76e9b"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "dba5fdee6eab073912fb38e690c776f48e0e4884"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f"
   },
   "outputs": [],
   "source": [
    "#Checking files in Kaggle\n",
    "# List data files that are connected to the kernel\n",
    "\n",
    "#os.listdir('../input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0"
   },
   "outputs": [],
   "source": [
    "# Read Train & Test Files\n",
    "\n",
    "#Kaggle\n",
    "#train_file = bz2.BZ2File('../input/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost\n",
    "#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost - VersiÃ³n recortada del archivo\n",
    "train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n",
    "\n",
    "#Create Lists containing Train & Test sentences\n",
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()\n",
    "\n",
    "#Convert from raw binary strings to strings that can be parsed\n",
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file, test_file\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de elementos del Training Set: 30000\n",
      "Cantidad de elementos del Testing Set: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\n",
    "print(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd"
   },
   "outputs": [],
   "source": [
    "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "\n",
    "# Make everything Lower Case\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
   },
   "source": [
    "## Checking data before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_file_lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a801c323d527>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Random\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file_lines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Before\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data before cleaning:\\n{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file_lines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_file_lines' is not defined"
     ]
    }
   ],
   "source": [
    "#Random\n",
    "r = random.randint(1,len(train_file_lines))\n",
    "\n",
    "#Before\n",
    "print(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n",
    "\n",
    "#After\n",
    "print(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n",
    "\n",
    "#Labels\n",
    "print(\"\\nLabel:{}\".format(train_labels[r-1:r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
   },
   "source": [
    "### Output\n",
    "From the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n",
    "\n",
    "So, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file_lines, test_file_lines\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
   },
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Delete special characters -> In Keras I use the Filter.\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub(\"[^a-zA-Z]\", \" \",train_sentences[i])\n",
    "    \n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub(\"[^a-zA-Z]\", \" \",train_sentences[i])\n",
    "    \n",
    "#Base definitions for text preprocessing\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff"
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "v = CountVectorizer(analyzer = \"word\",max_features = max_features)\n",
    "\n",
    "X_train = v.fit_transform(train_sentences)\n",
    "X_test = v.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5381cf4aa759c3cde8c1a3bc775c71eaa0debfae"
   },
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "034188709597cb9a4390840f8b4010b8e66a5b13"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create a validation dataset\n",
    "validation_size = 0.2\n",
    "X_train, X_valid, train_labels, valid_labels = train_test_split(X_train, train_labels, test_size = validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "9ba4e755b369728f6b3ec6007c5bbb117ba098f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Armo los distintos clasificadores que voy a utilizar -> Revisar posibles hiperparametros como lo hice en NN.\n",
    "Classifiers = [\n",
    "    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n",
    "    #GaussianNB(),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    #DecisionTreeClassifier(),\n",
    "    #RandomForestClassifier(n_estimators=200),\n",
    "    AdaBoostClassifier(),\n",
    "    #KNeighborsClassifier(3),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64ea1632ab83398264b37480e87db3e4f5b60a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegressionis 0.7073333333333334\n"
     ]
    }
   ],
   "source": [
    "Accuracy=[]\n",
    "Model=[]\n",
    "\n",
    "#Los que tira error hay que hacerles la transformacion de sparse matrix a dense matrix (Random forest era uno que tiraba error).\n",
    "\n",
    "\n",
    "train_labels_array = np.array(train_labels)\n",
    "valid_labels_array = np.array(valid_labels)\n",
    "\n",
    "for classifier in Classifiers:\n",
    "    fit = classifier.fit(X_train,train_labels_array)\n",
    "    pred = fit.predict(X_valid)\n",
    "    accuracy = accuracy_score(pred,valid_labels_array)\n",
    "    Accuracy.append(accuracy)\n",
    "    Model.append(classifier.__class__.__name__)\n",
    "    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1e6c3d5a86ee18206ac22f67ad2641962269ac6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8272ef6108d2803125980a8cbed4c5ae0be17c36"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a77de22781f477c899432c0a97324dbbc1b76e9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "dba5fdee6eab073912fb38e690c776f48e0e4884"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f"
   },
   "outputs": [],
   "source": [
    "#Checking files in Kaggle\n",
    "# List data files that are connected to the kernel\n",
    "\n",
    "#os.listdir('../input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0"
   },
   "outputs": [],
   "source": [
    "# Read Train & Test Files\n",
    "\n",
    "#Kaggle\n",
    "#train_file = bz2.BZ2File('../input/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost\n",
    "#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost - VersiÃ³n recortada del archivo\n",
    "train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n",
    "\n",
    "#Create Lists containing Train & Test sentences\n",
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()\n",
    "\n",
    "#Convert from raw binary strings to strings that can be parsed\n",
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file, test_file\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de elementos del Training Set: 30000\n",
      "Cantidad de elementos del Testing Set: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\n",
    "print(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd"
   },
   "outputs": [],
   "source": [
    "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "\n",
    "# Make everything Lower Case\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
   },
   "source": [
    "## Checking data before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before cleaning:\n",
      "['__label__2 air ratchet: air ratchet workes on 4cfms of air witch is about all my compressor can handle. it has good speed and power also workes in tight quarters witch is what i need when i work on my equipment.couple that with good service and quick delivery. works for me.\\r\\n']\n",
      "\n",
      "Data after cleaning:\n",
      "['air ratchet: air ratchet workes on 0cfms of air witch is about all my compressor can handle. it has good speed and power also workes in tight quarters witch is what i need when i work on my equipment.couple that with good service and quick delivery. works for me.\\r']\n",
      "\n",
      "Label:[1]\n"
     ]
    }
   ],
   "source": [
    "#Random\n",
    "r = random.randint(1,len(train_file_lines))\n",
    "\n",
    "#Before\n",
    "print(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n",
    "\n",
    "#After\n",
    "print(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n",
    "\n",
    "#Labels\n",
    "print(\"\\nLabel:{}\".format(train_labels[r-1:r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
   },
   "source": [
    "### Output\n",
    "From the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n",
    "\n",
    "So, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file_lines, test_file_lines\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
   },
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "#Base definitions for text preprocessing\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff"
   },
   "outputs": [],
   "source": [
    "#Tokenizer definition\n",
    "#Filtro caracteres especiales usando el Tokenizer de keras.\n",
    "tokenizer = text.Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "\n",
    "#Fit on text -> Only the train dataset !!!\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "#Training set\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "\n",
    "#Test set\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "becc7ba40bc713cec836c5ae061bc6553c4342dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  822,   127,  4020,    10,     1,  1010,  4336,     2,     3,\n",
       "         171,   144,     8,     9,     4,    33,    17,    20,  6766,\n",
       "          12,     3,   325,   143,   157,    25,    85,   357,     3,\n",
       "          14,  2283,  1747,     2,  6111,    50,     1,  1226,  9909,\n",
       "        8492,    51,    33,  2165,  3276,    51,  5288, 14989, 18055,\n",
       "        6238,    20,  2355, 19489,   775,    27,   447,    19,    51,\n",
       "         295,   272,  1752,  1493,    82,   465,     4,  1080,     4,\n",
       "         183,   383,  1080,    23,     4,   183,  6297,  4020,  8202,\n",
       "       11501,    51,  2389,   145,    10,  6766,     2,    31,    12,\n",
       "          55,    49,     6,   177,  3206,     2,  3292,   177,    20,\n",
       "         510,     5,  9266,    51,   764,     9,    16,    10,  7012,\n",
       "          11])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print a random matrix\n",
    "X_train[r]\n",
    "# summarize what was learned -> Si quiero ver el tokenizer que aprendio usando los 2 parametros (Max_features,max_length)\n",
    "#print(t.word_counts)\n",
    "#print(t.document_count)\n",
    "#print(t.word_index)\n",
    "#print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5381cf4aa759c3cde8c1a3bc775c71eaa0debfae"
   },
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "034188709597cb9a4390840f8b4010b8e66a5b13"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create a validation dataset\n",
    "validation_size = 0.2\n",
    "X_train, X_valid, train_labels, valid_labels = train_test_split(X_train, train_labels, test_size = validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f9c36b66bfc3f7cfe53c50505956e443b643c319"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "9ba4e755b369728f6b3ec6007c5bbb117ba098f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Armo los distintos clasificadores que voy a utilizar -> Revisar posibles hiperparametros como lo hice en NN.\n",
    "Classifiers = [\n",
    "    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n",
    "    GaussianNB()\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    #DecisionTreeClassifier(),\n",
    "    #RandomForestClassifier(n_estimators=200),\n",
    "    #AdaBoostClassifier(),\n",
    "    #KNeighborsClassifier(3),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd2138fd181f6d8302bc5d5697453bbdc8ff3f98"
   },
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "64ea1632ab83398264b37480e87db3e4f5b60a34"
   },
   "outputs": [],
   "source": [
    "Accuracy=[]\n",
    "Model=[]\n",
    "for classifier in Classifiers:\n",
    "    fit = classifier.fit(X_train,train_labels)\n",
    "    pred = fit.predict(X_valid)\n",
    "    accuracy = accuracy_score(pred,valid_labels)\n",
    "    Accuracy.append(accuracy)\n",
    "    Model.append(classifier.__class__.__name__)\n",
    "    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

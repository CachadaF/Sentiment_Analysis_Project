{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8272ef6108d2803125980a8cbed4c5ae0be17c36"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a77de22781f477c899432c0a97324dbbc1b76e9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "dba5fdee6eab073912fb38e690c776f48e0e4884"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f"
   },
   "outputs": [],
   "source": [
    "#Checking files in Kaggle\n",
    "# List data files that are connected to the kernel\n",
    "\n",
    "#os.listdir('../input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0"
   },
   "outputs": [],
   "source": [
    "# Read Train & Test Files\n",
    "\n",
    "#Kaggle\n",
    "#train_file = bz2.BZ2File('../input/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost\n",
    "#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost - VersiÃ³n recortada del archivo\n",
    "train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n",
    "\n",
    "#Create Lists containing Train & Test sentences\n",
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()\n",
    "\n",
    "#Convert from raw binary strings to strings that can be parsed\n",
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file, test_file\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de elementos del Training Set: 30000\n",
      "Cantidad de elementos del Testing Set: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\n",
    "print(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd"
   },
   "outputs": [],
   "source": [
    "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "\n",
    "# Make everything Lower Case\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
   },
   "source": [
    "## Checking data before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before cleaning:\n",
      "['__label__2 Garbage is great!: Garbage\\'s first album was a without-a-doubt 5 star, probably my favorite album ever. Version 2.0 is really good, but just a notch under the first. Still it is pretty darn good! The album doesn\\'t have super hits like \"Stupid Girl\" \"Queer\" or \"Only Happy When It Rains,\" but it does have good songs. My favorite, which sounds like the Garbage most people know, is \"Temptation Waits.\" \"Hammering in my Head\" is great, too. The most popular ones, \"Push It\" and \"I Think I\\'m Paranoid,\" are popular for a reason, too. Overall, it is a great album.\\r\\n']\n",
      "\n",
      "Data after cleaning:\n",
      "['garbage is great!: garbage\\'s first album was a without-a-doubt 0 star, probably my favorite album ever. version 0.0 is really good, but just a notch under the first. still it is pretty darn good! the album doesn\\'t have super hits like \"stupid girl\" \"queer\" or \"only happy when it rains,\" but it does have good songs. my favorite, which sounds like the garbage most people know, is \"temptation waits.\" \"hammering in my head\" is great, too. the most popular ones, \"push it\" and \"i think i\\'m paranoid,\" are popular for a reason, too. overall, it is a great album.\\r']\n",
      "\n",
      "Label:[1]\n"
     ]
    }
   ],
   "source": [
    "#Random\n",
    "r = random.randint(1,len(train_file_lines))\n",
    "\n",
    "#Before\n",
    "print(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n",
    "\n",
    "#After\n",
    "print(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n",
    "\n",
    "#Labels\n",
    "print(\"\\nLabel:{}\".format(train_labels[r-1:r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
   },
   "source": [
    "### Output\n",
    "From the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n",
    "\n",
    "So, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file_lines, test_file_lines\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
   },
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Delete special characters -> In Keras I use the Filter.\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub(\"[^a-zA-Z]\", \" \",train_sentences[i])\n",
    "    \n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub(\"[^a-zA-Z]\", \" \",train_sentences[i])\n",
    "    \n",
    "#Base definitions for text preprocessing\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff"
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "v = CountVectorizer(analyzer = \"word\",max_features = max_features)\n",
    "\n",
    "X_train = v.fit_transform(train_sentences)\n",
    "X_test = v.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5381cf4aa759c3cde8c1a3bc775c71eaa0debfae"
   },
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "034188709597cb9a4390840f8b4010b8e66a5b13"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create a validation dataset\n",
    "validation_size = 0.2\n",
    "X_train, X_valid, train_labels, valid_labels = train_test_split(X_train, train_labels, test_size = validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "9ba4e755b369728f6b3ec6007c5bbb117ba098f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_labels_array = np.array(train_labels)\n",
    "valid_labels_array = np.array(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c1e6c3d5a86ee18206ac22f67ad2641962269ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression is 0.879\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=200)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_valid)\n",
    "accuracy = accuracy_score(pred,valid_labels_array)\n",
    "\n",
    "print('Accuracy of Logistic Regression is '+str(accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVC(SVM) is 0.8531666666666666\n"
     ]
    }
   ],
   "source": [
    "classifier = LinearSVC(max_iter=1000)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_valid)\n",
    "accuracy = accuracy_score(pred,valid_labels_array)\n",
    "\n",
    "print('Accuracy of Linear SVC is '+str(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NuSVC is 0.7295\n"
     ]
    }
   ],
   "source": [
    "classifier = NuSVC(max_iter=1000)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_valid)\n",
    "accuracy = accuracy_score(pred,valid_labels_array)\n",
    "\n",
    "print('Accuracy of NuSVC is '+str(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NuSVC is 0.8545\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_valid)\n",
    "accuracy = accuracy_score(pred,valid_labels_array)\n",
    "\n",
    "print('Accuracy of Random Forest Classifier is '+str(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = AdaBoostClassifier()\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_valid)\n",
    "accuracy = accuracy_score(pred,valid_labels_array)\n",
    "\n",
    "print('Accuracy ofAdaBoostClassifier is '+str(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(3)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_valid)\n",
    "accuracy = accuracy_score(pred,valid_labels_array)\n",
    "\n",
    "print('Accuracy of KNeighborsClassifier is '+str(accuracy)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

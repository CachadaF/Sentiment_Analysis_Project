{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8272ef6108d2803125980a8cbed4c5ae0be17c36"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "a77de22781f477c899432c0a97324dbbc1b76e9b"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "dba5fdee6eab073912fb38e690c776f48e0e4884"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f"
   },
   "outputs": [],
   "source": [
    "#Checking files in Kaggle\n",
    "# List data files that are connected to the kernel\n",
    "\n",
    "#os.listdir('../input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0"
   },
   "outputs": [],
   "source": [
    "# Read Train & Test Files\n",
    "\n",
    "#Kaggle\n",
    "#train_file = bz2.BZ2File('../input/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost\n",
    "#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost - VersiÃ³n recortada del archivo\n",
    "train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n",
    "\n",
    "#Create Lists containing Train & Test sentences\n",
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()\n",
    "\n",
    "#Convert from raw binary strings to strings that can be parsed\n",
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file, test_file\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de elementos del Training Set: 30000\n",
      "Cantidad de elementos del Testing Set: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\n",
    "print(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd"
   },
   "outputs": [],
   "source": [
    "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "\n",
    "# Make everything Lower Case\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
   },
   "source": [
    "## Checking data before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before cleaning:\n",
      "[\"__label__1 Great Story ... Bad writing: So sue me ... I just don't like Nathanial Hawthorne. I think the plot is brilliant, but I find his method of telling the story annoying. I'm afraid this isn't an assertion I can back up with a lot of concrete examples, it's just that his writing gives me a vague feeling of malaise. I find this true of his short stories as well, so I never bothered with the other novels. But you'll have to read The Scarlet Letter for school anyway, so borrow it from the library.\\r\\n\"]\n",
      "\n",
      "Data after cleaning:\n",
      "[\"great story ... bad writing: so sue me ... i just don't like nathanial hawthorne. i think the plot is brilliant, but i find his method of telling the story annoying. i'm afraid this isn't an assertion i can back up with a lot of concrete examples, it's just that his writing gives me a vague feeling of malaise. i find this true of his short stories as well, so i never bothered with the other novels. but you'll have to read the scarlet letter for school anyway, so borrow it from the library.\\r\"]\n",
      "\n",
      "Label:[0]\n"
     ]
    }
   ],
   "source": [
    "#Random\n",
    "r = random.randint(1,len(train_file_lines))\n",
    "\n",
    "#Before\n",
    "print(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n",
    "\n",
    "#After\n",
    "print(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n",
    "\n",
    "#Labels\n",
    "print(\"\\nLabel:{}\".format(train_labels[r-1:r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
   },
   "source": [
    "### Output\n",
    "From the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n",
    "\n",
    "So, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file_lines, test_file_lines\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
   },
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Delete special characters -> In Keras I use the Filter.\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub(\"[^a-zA-Z]\", \" \",train_sentences[i])\n",
    "    \n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub(\"[^a-zA-Z]\", \" \",train_sentences[i])\n",
    "    \n",
    "#Base definitions for text preprocessing\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff"
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "v = CountVectorizer(analyzer = \"word\",max_features = max_features)\n",
    "\n",
    "X_train = v.fit_transform(train_sentences)\n",
    "X_test = v.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "9ba4e755b369728f6b3ec6007c5bbb117ba098f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_labels_array = np.array(train_labels)\n",
    "test_labels_array = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "c1e6c3d5a86ee18206ac22f67ad2641962269ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression is 0.5052\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=200)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_test)\n",
    "accuracy = accuracy_score(pred,test_labels_array)\n",
    "\n",
    "print('Accuracy of Logistic Regression is '+str(accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVC is 0.5039\n"
     ]
    }
   ],
   "source": [
    "classifier = LinearSVC(max_iter=1000)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_test)\n",
    "accuracy = accuracy_score(pred,test_labels_array)\n",
    "\n",
    "print('Accuracy of Linear SVC is '+str(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NuSVC(max_iter=1000)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_test)\n",
    "accuracy = accuracy_score(pred,test_labels_array)\n",
    "\n",
    "print('Accuracy of NuSVC is '+str(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NuSVC is 0.8545\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_test)\n",
    "accuracy = accuracy_score(pred,test_labels_array)\n",
    "\n",
    "print('Accuracy of Random Forest Classifier is '+str(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = AdaBoostClassifier()\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_test)\n",
    "accuracy = accuracy_score(pred,test_labels_array)\n",
    "\n",
    "print('Accuracy ofAdaBoostClassifier is '+str(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(3)\n",
    "\n",
    "fit = classifier.fit(X_train,train_labels_array)\n",
    "pred = fit.predict(X_test)\n",
    "accuracy = accuracy_score(pred,test_labels_array)\n",
    "\n",
    "print('Accuracy of KNeighborsClassifier is '+str(accuracy)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "%load_ext autoreload\n%autoreload 2",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "4dd662d39b7605517f4e8d44f1925e4b7559f61e"
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport chardet\nimport re\nimport os\nimport random",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
      },
      "cell_type": "markdown",
      "source": "# Pre-processing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f"
      },
      "cell_type": "code",
      "source": "#Checking files in Kaggle\n# List data files that are connected to the kernel\nos.listdir('../input')",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "['test.ft.txt.bz2', 'train.ft.txt.bz2']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Read Train & Test Files\n\n#Kaggle\ntrain_file = bz2.BZ2File('../input/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n\n#Localhost\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n\n#Localhost - Versión recortada del archivo\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n\n#Create Lists containing Train & Test sentences\ntrain_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()\n\n#Convert from raw binary strings to strings that can be parsed\ntrain_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file, test_file\n#Garbage collector\ngc.collect()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2"
      },
      "cell_type": "code",
      "source": "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\nprint(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Cantidad de elementos del Training Set: 3600000\nCantidad de elementos del Testing Set: 400000\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
      },
      "cell_type": "markdown",
      "source": "## Clean data"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd"
      },
      "cell_type": "code",
      "source": "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n\n# Make everything Lower Case\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n\n# Modify URLs to <url>\nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
      },
      "cell_type": "markdown",
      "source": "## Checking data before and after cleaning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d"
      },
      "cell_type": "code",
      "source": "#Random\nr = random.randint(1,len(train_file_lines))\n\n#Before\nprint(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n\n#After\nprint(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n\n#Labels\nprint(\"\\nLabel:{}\".format(train_labels[r-1:r]))",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Data before cleaning:\n['__label__2 great book: This book has almost all of the information you need to build you own barn and fencing. I would recomend it!\\n']\n\nData after cleaning:\n['great book: this book has almost all of the information you need to build you own barn and fencing. i would recomend it!']\n\nLabel:[1]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
      },
      "cell_type": "markdown",
      "source": "### Output\nFrom the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n\nSo, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file_lines, test_file_lines\n#Garbage collector\ngc.collect()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
      },
      "cell_type": "markdown",
      "source": "## Text Pre-processing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing import text, sequence\n\n#Base definitions for text preprocessing\nmax_features = 20000\nmaxlen = 100",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff"
      },
      "cell_type": "code",
      "source": "#Tokenizer definition\n#Filtro caracteres especiales usando el Tokenizer de keras.\ntokenizer = text.Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n\n#Fit on text -> Only the train dataset !!!\ntokenizer.fit_on_texts(train_sentences)\n\n#Training set\ntokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n\n#Test set\ntokenized_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "becc7ba40bc713cec836c5ae061bc6553c4342dc"
      },
      "cell_type": "code",
      "source": "#Print a random matrix\nX_train[r]\n# summarize what was learned -> Si quiero ver el tokenizer que aprendio usando los 2 parametros (Max_features,max_length)\n#print(t.word_counts)\n#print(t.document_count)\n#print(t.word_index)\n#print(t.word_docs)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,  1263,    37,     1,  2003,\n        4185,     3,    93,    10,  9852,     2,    20,   312,     1,\n        6435,   112,   206,     6,     9,   472,     4, 12491,  2110,\n        3579,    16,     1, 11026,   273,   885,  1180,    10,     8,\n         698,     9,  3708,   695,    10,  1420,  3815,   202,   523,\n         970,    35,  2862,  3309,     2,    11,    43,     1,   915,\n         450], dtype=int32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5381cf4aa759c3cde8c1a3bc775c71eaa0debfae"
      },
      "cell_type": "markdown",
      "source": "### Validation dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "034188709597cb9a4390840f8b4010b8e66a5b13",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n# Create a validation dataset\nvalidation_size = 0.2\nX_train, X_valid, train_labels, valid_labes = train_test_split(X_train, train_labels, test_size = validation_size)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9c36b66bfc3f7cfe53c50505956e443b643c319"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences\n#Garbage collector\ngc.collect()",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
      },
      "cell_type": "markdown",
      "source": "## Model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "666878741c1705787c1e9f6d3cc882ee3000b6c5"
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input\nfrom keras.layers.embeddings import Embedding",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f352c6d9b432671718da6f2d64083796924ebb28"
      },
      "cell_type": "code",
      "source": "#Defino los parametros del modelo:\np = 0.10 #Dropout\nlr = 0.0001 #Learning Rate\nbatch_size = 2048\nepochs = 4 #Bajamos de 10 -> 4.\n\n#Embedding size -> Ver para que sirve, todavia falta entenderlo?\nembed_size = 128\n#CNN_Filters\nCNN_Filters = embed_size * 2\n#RNN\nRNN_Neurons = 128\ntime_steps = 0",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3e60f578e04bea6df79525c9542e5dec03ac19d",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Creo el modelo\ndef model():\n    #input\n    inp = Input(shape=(maxlen, ))\n    #Embedding\n    x = Embedding(max_features, embed_size)(inp)\n    x = Dropout(0.25)(x)\n    #CNN\n    x = Convolution1D(2*embed_size, kernel_size = 3)(x)\n    x = Convolution1D(2*embed_size, kernel_size = 3)(x)\n    #RNN\n    x_a = GRU(512)(x)  \n    x_b = GRU(512)(x)  \n    #Concatenate\n    x = concatenate([x_a, x_b])\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    #Input - Output\n    model = Model(inputs=inp, outputs=x)\n    #Optimizer\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n\n    return model\n\nmodel = model()\nmodel.summary()\n\n#Optimizers\n\n#Accuracy en metrics es más generico, y depende de la LOSS.\n#model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['accuracy'])",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 100)          0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 100, 128)     2560000     input_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 100, 128)     0           embedding_1[0][0]                \n__________________________________________________________________________________________________\nconv1d_1 (Conv1D)               (None, 98, 256)      98560       dropout_1[0][0]                  \n__________________________________________________________________________________________________\nconv1d_2 (Conv1D)               (None, 96, 256)      196864      conv1d_1[0][0]                   \n__________________________________________________________________________________________________\ngru_1 (GRU)                     (None, 512)          1181184     conv1d_2[0][0]                   \n__________________________________________________________________________________________________\ngru_2 (GRU)                     (None, 512)          1181184     conv1d_2[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 1024)         0           gru_1[0][0]                      \n                                                                 gru_2[0][0]                      \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 1024)         0           concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 64)           65600       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            65          dropout_3[0][0]                  \n==================================================================================================\nTotal params: 5,283,457\nTrainable params: 5,283,457\nNon-trainable params: 0\n__________________________________________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a0611cfd88f87e629154694961c17ddf6e763002"
      },
      "cell_type": "code",
      "source": "# Callbacks\n\n## Callback para guardar pesos\ncheckpointer = ModelCheckpoint(filepath='Sentiment_Analysis_Amazon_Reviews.hdf5', monitor='val_loss'\n                                   ,verbose=1, save_best_only=True, mode='min')\ncallbacks = [checkpointer]",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "360004b4d48a606985a235a8f263b5457ab75bef"
      },
      "cell_type": "code",
      "source": "# Fit del modelo -> Usando todo el dataset\n\"\"\"\nmodel.fit(X_train,train_labels\n          ,epochs=epochs\n          ,batch_size = batch_size          \n          ,shuffle = True\n          ,validation_data = (X_valid,valid_labes)\n          ,callbacks=callbacks)\n\"\"\"\n\n# Fit del modelo -> Usando solo un fragmento del datasset\nmodel.fit(X_train[:100000], train_labels[:100000]\n          ,epochs=epochs\n          ,batch_size = batch_size          \n          ,shuffle = True\n          ,validation_split=0.20\n          ,callbacks=callbacks)",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 80000 samples, validate on 20000 samples\nEpoch 1/4\n80000/80000 [==============================] - 140s 2ms/step - loss: 0.6852 - binary_accuracy: 0.5705 - val_loss: 0.6510 - val_binary_accuracy: 0.6387\n\nEpoch 00001: val_loss improved from inf to 0.65097, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\nEpoch 2/4\n80000/80000 [==============================] - 132s 2ms/step - loss: 0.5290 - binary_accuracy: 0.7341 - val_loss: 0.3992 - val_binary_accuracy: 0.8212\n\nEpoch 00002: val_loss improved from 0.65097 to 0.39918, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\nEpoch 3/4\n80000/80000 [==============================] - 132s 2ms/step - loss: 0.3700 - binary_accuracy: 0.8389 - val_loss: 0.3550 - val_binary_accuracy: 0.8456\n\nEpoch 00003: val_loss improved from 0.39918 to 0.35495, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\nEpoch 4/4\n80000/80000 [==============================] - 133s 2ms/step - loss: 0.3179 - binary_accuracy: 0.8665 - val_loss: 0.3081 - val_binary_accuracy: 0.8719\n\nEpoch 00004: val_loss improved from 0.35495 to 0.30810, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f3b834a8518>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "fd2138fd181f6d8302bc5d5697453bbdc8ff3f98"
      },
      "cell_type": "markdown",
      "source": "## Test "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64ea1632ab83398264b37480e87db3e4f5b60a34"
      },
      "cell_type": "code",
      "source": "score, acc = model.evaluate(X_test, test_labels, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "400000/400000 [==============================] - 171s 428us/step\nTest score: 0.30853510454177857\nTest accuracy: 0.8726525\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "eb8e0111fcd782ec13d69743ef021ad75fdcc791"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
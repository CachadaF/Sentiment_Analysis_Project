{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f"
      },
      "cell_type": "code",
      "source": "%load_ext autoreload\n%autoreload 2",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "4dd662d39b7605517f4e8d44f1925e4b7559f61e"
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport chardet\nimport re\nimport os\nimport random",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
      },
      "cell_type": "markdown",
      "source": "# Pre-processing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f"
      },
      "cell_type": "code",
      "source": "#Checking files in Kaggle\n# List data files that are connected to the kernel\nos.listdir('../input')",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "['test.ft.txt.bz2', 'train.ft.txt.bz2']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2385eb7771d4c6dee048637dd74fab19b2a4f33b"
      },
      "cell_type": "code",
      "source": "os.listdir('../input')",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "['test.ft.txt.bz2', 'train.ft.txt.bz2']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0"
      },
      "cell_type": "code",
      "source": "# Read Train & Test Files\n\n#Kaggle\ntrain_file = bz2.BZ2File('../input/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n\n#Localhost\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n\n#Localhost - VersiÃ³n recortada del archivo\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n\n#Create Lists containing Train & Test sentences\ntrain_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()\n\n#Convert from raw binary strings to strings that can be parsed\ntrain_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc15304e601110eb2244b6d3ee9299829742cd31"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file, test_file\n#Garbage collector\ngc.collect()",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2"
      },
      "cell_type": "code",
      "source": "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\nprint(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Cantidad de elementos del Training Set: 3600000\nCantidad de elementos del Testing Set: 400000\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
      },
      "cell_type": "markdown",
      "source": "## Clean data"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd"
      },
      "cell_type": "code",
      "source": "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n\n# Make everything Lower Case\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n\n# Modify URLs to <url>\nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
      },
      "cell_type": "markdown",
      "source": "## Checking data before and after cleaning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d"
      },
      "cell_type": "code",
      "source": "#Random\nr = random.randint(1,len(train_file_lines))\n\n#Before\nprint(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n\n#After\nprint(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n\n#Labels\nprint(\"\\nLabel:{}\".format(train_labels[r-1:r]))",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Data before cleaning:\n[\"__label__1 HOPE SINKS!!!: This movie was so boring, I almost didn't finish it! It doesn't make you care about the characters and what happens to them. The story had potential but the movie just dragged. I was disappointed that Sandra Bullock and Harry Connick Jr (a potentially great team) would do such a movie. Don't waste your money on this one!\\n\"]\n\nData after cleaning:\n[\"hope sinks!!!: this movie was so boring, i almost didn't finish it! it doesn't make you care about the characters and what happens to them. the story had potential but the movie just dragged. i was disappointed that sandra bullock and harry connick jr (a potentially great team) would do such a movie. don't waste your money on this one!\"]\n\nLabel:[0]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
      },
      "cell_type": "markdown",
      "source": "### Output\nFrom the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n\nSo, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d7ed6c86b87ebb2bf7e89cebbdaac9b9f8d2af1"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file_lines, test_file_lines\n#Garbage collector\ngc.collect()",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
      },
      "cell_type": "markdown",
      "source": "## Text Pre-processing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing import text, sequence\n\n#Base definitions for text preprocessing\nmax_features = 20000\nmaxlen = 100",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff"
      },
      "cell_type": "code",
      "source": "#Tokenizer definition\n#Filtro caracteres especiales usando el Tokenizer de keras.\ntokenizer = text.Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n\n#Fit on text -> Only the train dataset !!!\ntokenizer.fit_on_texts(train_sentences)\n\n#Training set\ntokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n\n#Test set\ntokenized_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "becc7ba40bc713cec836c5ae061bc6553c4342dc"
      },
      "cell_type": "code",
      "source": "#Print a random matrix\nX_train[r]\n# summarize what was learned -> Si quiero ver el tokenizer que aprendio usando los 2 parametros (Max_features,max_length)\n#print(t.word_counts)\n#print(t.document_count)\n#print(t.word_index)\n#print(t.word_docs)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n         837,  1750,     8,     9,    36,  1066,  1598,     1,   106,\n           9,   134,     2,    58,    23,    54,   104,  1984,     5,\n         120,     6,   921, 12533,   997,    67,   232,  6290,  1393,\n          16,    44,    67,   191,  1359,    84,     1,   198,   553,\n         200,     9,   274,    56,     3,   140,  4567,  9863,    11,\n          90,    97,    13,  9042,    11,    42,  5078,  3176,   296,\n         138,   334,     9,   169,    18,     1,   894,     2,     1,\n        1010,     9,   232,    32,    16,     8,     9,    36,     4,\n          58,    62,    14,    84,    32,    14,    84,   128,    36,\n          58], dtype=int32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5381cf4aa759c3cde8c1a3bc775c71eaa0debfae"
      },
      "cell_type": "markdown",
      "source": "### Validation dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "034188709597cb9a4390840f8b4010b8e66a5b13"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n# Create a validation dataset\nvalidation_size = 0.2\nX_train, X_valid, train_labels, valid_labes = train_test_split(X_train, train_labels, test_size = validation_size)",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9318d8f866167029684b02deee374fb310339dd1"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences\n#Garbage collector\ngc.collect()",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "159"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
      },
      "cell_type": "markdown",
      "source": "## Model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "666878741c1705787c1e9f6d3cc882ee3000b6c5"
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.models import Sequential\nfrom keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization\nfrom keras.layers.embeddings import Embedding",
      "execution_count": 40,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f352c6d9b432671718da6f2d64083796924ebb28"
      },
      "cell_type": "code",
      "source": "#Defino los parametros del modelo:\np = 0.25 #Dropout\nlr = 0.0001 #Learning Rate\nbatch_size = 1024\nepochs = 10\n\n#Embedding size -> Ver para que sirve, todavia falta entenderlo?\nembed_size = 32",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "d3e60f578e04bea6df79525c9542e5dec03ac19d"
      },
      "cell_type": "code",
      "source": "# Creo el modelo\nmodel=Sequential()\n\n#Embedding\nmodel.add(Embedding(max_features, embed_size, input_length=maxlen))\n\n#CNN\nmodel.add(Convolution1D(filters=32, kernel_size=2, padding=\"same\", name='Conv1'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(p))\n\n#model.add(Convolution1D(filters=16, kernel_size=3, padding=\"same\", name='Conv2',input_shape=input_shape))\nmodel.add(Convolution1D(filters=16, kernel_size=2, padding=\"same\", name='Conv2'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(p))\n\nmodel.add(MaxPooling1D(pool_size=2, name='MaxPool1'))\nmodel.add(Dropout(p))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(32))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(p))\n\nmodel.add(Dense(1, activation='softmax'))\n\nmodel.summary()\n\n#Optimizers\nADAM = optimizers.Adam(lr=lr)\nmodel.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 100, 32)           640000    \n_________________________________________________________________\nConv1 (Conv1D)               (None, 100, 32)           2080      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 100, 32)           128       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 100, 32)           0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 100, 32)           0         \n_________________________________________________________________\nConv2 (Conv1D)               (None, 100, 16)           1040      \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 100, 16)           64        \n_________________________________________________________________\nactivation_2 (Activation)    (None, 100, 16)           0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 100, 16)           0         \n_________________________________________________________________\nMaxPool1 (MaxPooling1D)      (None, 50, 16)            0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 50, 16)            0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 800)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                25632     \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 32)                128       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 32)                0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 669,105\nTrainable params: 668,945\nNon-trainable params: 160\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a0611cfd88f87e629154694961c17ddf6e763002"
      },
      "cell_type": "code",
      "source": "# Callbacks\n\n## Callback para guardar pesos\ncheckpointer = ModelCheckpoint(filepath='CNN_Sentiment_Analysis_Amazon_Reviews.hdf5', monitor='val_loss'\n                                   ,verbose=1, save_best_only=True, mode='min')\ncallbacks = [checkpointer]",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "360004b4d48a606985a235a8f263b5457ab75bef"
      },
      "cell_type": "code",
      "source": "# Fit del modelo\nmodel.fit(X_train\n          ,train_labels\n          ,epochs=epochs\n          ,batch_size = batch_size          \n          ,shuffle = True\n          ,validation_data = (X_valid,valid_labes)\n          #,validation_split=0.20 -> Para no separar en validation antes.\n          ,callbacks=callbacks)\n",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 2880000 samples, validate on 720000 samples\nEpoch 1/10\n2880000/2880000 [==============================] - 98s 34us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00001: val_loss improved from inf to 7.95343, saving model to CNN_Sentiment_Analysis_Amazon_Reviews.hdf5\nEpoch 2/10\n2880000/2880000 [==============================] - 90s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00002: val_loss did not improve\nEpoch 3/10\n2880000/2880000 [==============================] - 90s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00003: val_loss did not improve\nEpoch 4/10\n2880000/2880000 [==============================] - 90s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00004: val_loss did not improve\nEpoch 5/10\n2880000/2880000 [==============================] - 90s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00005: val_loss did not improve\nEpoch 6/10\n2880000/2880000 [==============================] - 91s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00006: val_loss did not improve\nEpoch 7/10\n2880000/2880000 [==============================] - 90s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00007: val_loss did not improve\nEpoch 8/10\n2880000/2880000 [==============================] - 91s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00008: val_loss did not improve\nEpoch 9/10\n2880000/2880000 [==============================] - 90s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00009: val_loss did not improve\nEpoch 10/10\n2880000/2880000 [==============================] - 90s 31us/step - loss: 7.9756 - binary_accuracy: 0.4997 - val_loss: 7.9534 - val_binary_accuracy: 0.5011\n\nEpoch 00010: val_loss did not improve\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f589de1abe0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "fd2138fd181f6d8302bc5d5697453bbdc8ff3f98"
      },
      "cell_type": "markdown",
      "source": "## Test "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64ea1632ab83398264b37480e87db3e4f5b60a34"
      },
      "cell_type": "code",
      "source": "score, acc = model.evaluate(X_test, test_labels, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "400000/400000 [==============================] - 4s 9us/step\nTest score: 7.971192351379394\nTest accuracy: 0.5\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "acef6b1a5a1a67b835411a82d7e5e16c6290ec0a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
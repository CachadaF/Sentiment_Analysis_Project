{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8272ef6108d2803125980a8cbed4c5ae0be17c36"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a77de22781f477c899432c0a97324dbbc1b76e9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a88e005a021fb7902fbda4c8f0acff1ffa04664d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input -> embed_size,maxlen,max_features,lr\n",
    "#Output -> model()\n",
    "\n",
    "#Multilayer - CNN - RNN model\n",
    "def get_model_1(embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Convolution1D(2*embed_size, kernel_size = 3)(x)\n",
    "    prefilt = Convolution1D(2*embed_size, kernel_size = 3)(x)\n",
    "    x = prefilt\n",
    "    for strides in [1, 1, 2]:\n",
    "        x = Convolution1D(128*2**(strides), strides = strides, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_size=3, kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x_f = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n",
    "    x_b = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x = concatenate([x_f, x_b])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#MLP Model\n",
    "def get_model_2 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Dense(128, activation=\"relu\")(x)    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#CNN Model\n",
    "def get_model_3 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#2 layers with Dropout - CNN Model\n",
    "def get_model_4 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#3 layers with Dropout - CNN Model\n",
    "def get_model_5 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Convolution1D(embed_size, kernel_size = 3, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#Simple RNN with Dropout\n",
    "def get_model_6 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = SimpleRNN(256) (x)\n",
    "    x = Dropout(0.10)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#Simple LSTM with Dropout\n",
    "def get_model_7 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = CuDNNLSTM(256) (x)\n",
    "    x = Dropout(0.10)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#Simple GRU with Dropout\n",
    "def get_model_8 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = CuDNNGRU(256) (x)\n",
    "    x = Dropout(0.10)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#CNN-RNN model with Dropout\n",
    "def get_model_9 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.10)(x)\n",
    "    x = CuDNNLSTM(256) (x)\n",
    "    x = Dropout(0.10)(x)    \n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#RNN-CNN model with Dropout\n",
    "def get_model_10 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = CuDNNLSTM(256, return_sequences=True) (x)\n",
    "    x = Dropout(0.10)(x)   \n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)   \n",
    "    x = Dropout(0.10)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#CNN-RNN model with Concatenate and Dropout\n",
    "def get_model_11 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    #Branch A\n",
    "    x_a = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)\n",
    "    x_a = Dropout(0.10)(x_a)\n",
    "    x_a = CuDNNLSTM(512)(x_a)  \n",
    "    #Branch B\n",
    "    x_b = Convolution1D(embed_size, kernel_size = 2, activation = \"relu\")(x)\n",
    "    x_b = Dropout(0.10)(x_b)\n",
    "    x_b = CuDNNLSTM(256, return_sequences=True)(x_b)  \n",
    "    x_b = Dropout(0.10)(x_b)\n",
    "    x_b = CuDNNLSTM(128)(x_b)  \n",
    "    #Concatenate Branch A-B\n",
    "    x = concatenate([x_a, x_b])\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "#CNN-RNN-MLP model with Concatenate and Dropout\n",
    "def get_model_12 (embed_size,maxlen,max_features,lr):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    #Branch A - 2 layers CNN + MLP\n",
    "    x_a = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)\n",
    "    x_a = Dropout(0.10)(x_a)\n",
    "    x_a = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)\n",
    "    x_a = Dropout(0.10)(x_a)\n",
    "    x_a = Dense(512, activation=\"relu\")(x_a)\n",
    "    x_a = Flatten()(x_a)\n",
    "    #Branch B - RNN + MLP\n",
    "    x_b = Convolution1D(embed_size, kernel_size = 2, activation = \"relu\")(x)\n",
    "    x_b = Dropout(0.10)(x_b)\n",
    "    x_b = CuDNNLSTM(256, return_sequences=True)(x_b)  \n",
    "    x_b = Dropout(0.10)(x_b)\n",
    "    x_b = Dense(512, activation=\"relu\")(x_b)\n",
    "    x_b = Flatten()(x_b)\n",
    "    #Concatenate Branch A-B\n",
    "    x = concatenate([x_a, x_b])\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    ADAM = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "dba5fdee6eab073912fb38e690c776f48e0e4884",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.ft.txt.bz2', 'train.ft.txt.bz2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking files in Kaggle\n",
    "# List data files that are connected to the kernel\n",
    "os.listdir('../input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Train & Test Files\n",
    "\n",
    "#Kaggle\n",
    "train_file = bz2.BZ2File('../input/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost\n",
    "#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n",
    "\n",
    "#Localhost - Versión recortada del archivo\n",
    "#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n",
    "#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n",
    "\n",
    "#Create Lists containing Train & Test sentences\n",
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()\n",
    "\n",
    "#Convert from raw binary strings to strings that can be parsed\n",
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file, test_file\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de elementos del Training Set: 3600000\n",
      "Cantidad de elementos del Testing Set: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\n",
    "print(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "\n",
    "# Make everything Lower Case\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
   },
   "source": [
    "## Checking data before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before cleaning:\n",
      "['__label__2 Good shape but a a little worn: My book was in good shape but a little worn. Not quite the \"like new\" I expected but still worth the price.\\n']\n",
      "\n",
      "Data after cleaning:\n",
      "['good shape but a a little worn: my book was in good shape but a little worn. not quite the \"like new\" i expected but still worth the price.']\n",
      "\n",
      "Label:[1]\n"
     ]
    }
   ],
   "source": [
    "#Random\n",
    "r = random.randint(1,len(train_file_lines))\n",
    "\n",
    "#Before\n",
    "print(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n",
    "\n",
    "#After\n",
    "print(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n",
    "\n",
    "#Labels\n",
    "print(\"\\nLabel:{}\".format(train_labels[r-1:r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
   },
   "source": [
    "### Output\n",
    "From the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n",
    "\n",
    "So, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del train_file_lines, test_file_lines\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
   },
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "#Base definitions for text preprocessing\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizer definition\n",
    "#Filtro caracteres especiales usando el Tokenizer de keras.\n",
    "tokenizer = text.Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "\n",
    "#Fit on text -> Only the train dataset !!!\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "#Training set\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "\n",
    "#Test set\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "becc7ba40bc713cec836c5ae061bc6553c4342dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1919,     2,   606,     1,  1211,   450,    22,     4,  2308,\n",
       "        3809, 19247,     2,  2763,   261,   152,    10,    79,   479,\n",
       "         158,    97,   111,    14,  4993,     5,    26,     4,   578,\n",
       "       12772, 16596,  9496,   335,   902,    79,   356,     2,  5251,\n",
       "           6,    17,    42,  3413,  1634,     2,   606,   169,     5,\n",
       "          50,   735,    12,    96,  5690,   824,     7,     1,  5690,\n",
       "          38, 14162,   102,   448,  2521,    37,    74,     1,   118,\n",
       "        1565,  2308, 16606,    61, 14565,  1220,     2,    42,   419,\n",
       "          38,   125,    37,    79,   921,  5298,     4,   215,   179,\n",
       "          50,    11,   207,  1347,    10,     1,  2308,  2143,  1018,\n",
       "           3,   395,    15,   279,     6,     9,    73,    22,     3,\n",
       "         101], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print a random matrix\n",
    "X_train[r]\n",
    "# summarize what was learned -> Si quiero ver el tokenizer que aprendio usando los 2 parametros (Max_features,max_length)\n",
    "#print(t.word_counts)\n",
    "#print(t.document_count)\n",
    "#print(t.word_index)\n",
    "#print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5381cf4aa759c3cde8c1a3bc775c71eaa0debfae"
   },
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "034188709597cb9a4390840f8b4010b8e66a5b13",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create a validation dataset\n",
    "validation_size = 0.2\n",
    "X_train, X_valid, train_labels, valid_labes = train_test_split(X_train, train_labels, test_size = validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "f9c36b66bfc3f7cfe53c50505956e443b643c319"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete memory reference (?)\n",
    "del tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences\n",
    "#Garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "666878741c1705787c1e9f6d3cc882ee3000b6c5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "f352c6d9b432671718da6f2d64083796924ebb28",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defino los parametros del modelo:\n",
    "lr = 0.0001 #Learning Rate\n",
    "batch_size = 1024\n",
    "epochs = 5\n",
    "embed_size = 128 #Embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "d3e60f578e04bea6df79525c9542e5dec03ac19d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_14 (Embedding)     (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100, 128)          16512     \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 12801     \n",
      "=================================================================\n",
      "Total params: 2,589,313\n",
      "Trainable params: 2,589,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# <<<<Models>>>>\n",
    "#Input -> get_model_1(embed_size,maxlen,lr)\n",
    "model = get_model_2(embed_size,maxlen,max_features,lr)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "a0611cfd88f87e629154694961c17ddf6e763002",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Callback para guardar pesos\n",
    "checkpointer = ModelCheckpoint(filepath='Sentiment_Analysis_Amazon_Reviews.hdf5', monitor='val_loss'\n",
    "                                   ,verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "360004b4d48a606985a235a8f263b5457ab75bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2880000 samples, validate on 720000 samples\n",
      "Epoch 1/5\n",
      "2880000/2880000 [==============================] - 69s 24us/step - loss: 0.3056 - binary_accuracy: 0.8665 - val_loss: 0.2541 - val_binary_accuracy: 0.8999\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25410, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\n",
      "Epoch 2/5\n",
      "2880000/2880000 [==============================] - 65s 22us/step - loss: 0.2497 - binary_accuracy: 0.9012 - val_loss: 0.2514 - val_binary_accuracy: 0.9010\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25410 to 0.25144, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\n",
      "Epoch 3/5\n",
      "2880000/2880000 [==============================] - 65s 22us/step - loss: 0.2456 - binary_accuracy: 0.9032 - val_loss: 0.2488 - val_binary_accuracy: 0.9019\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25144 to 0.24885, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\n",
      "Epoch 4/5\n",
      "2880000/2880000 [==============================] - 65s 22us/step - loss: 0.2422 - binary_accuracy: 0.9048 - val_loss: 0.2473 - val_binary_accuracy: 0.9029\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24885 to 0.24728, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\n",
      "Epoch 5/5\n",
      "2880000/2880000 [==============================] - 65s 22us/step - loss: 0.2396 - binary_accuracy: 0.9059 - val_loss: 0.2466 - val_binary_accuracy: 0.9032\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.24728 to 0.24664, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Fit del modelo -> Usando solo un fragmento del datasset\\nmodel.fit(X_train[:100000], train_labels[:100000]\\n          ,epochs=epochs\\n          ,batch_size = batch_size          \\n          ,shuffle = True\\n          ,validation_split=0.20\\n          ,callbacks=callbacks)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit del modelo -> Usando todo el dataset\n",
    "\n",
    "model.fit(X_train,train_labels\n",
    "          ,epochs=epochs\n",
    "          ,batch_size = batch_size          \n",
    "          ,shuffle = True\n",
    "          ,validation_data = (X_valid,valid_labes)\n",
    "          ,callbacks=callbacks)\n",
    "\"\"\"\n",
    "# Fit del modelo -> Usando solo un fragmento del datasset\n",
    "model.fit(X_train[:100000], train_labels[:100000]\n",
    "          ,epochs=epochs\n",
    "          ,batch_size = batch_size          \n",
    "          ,shuffle = True\n",
    "          ,validation_split=0.20\n",
    "          ,callbacks=callbacks)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd2138fd181f6d8302bc5d5697453bbdc8ff3f98"
   },
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "64ea1632ab83398264b37480e87db3e4f5b60a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000/400000 [==============================] - 3s 7us/step\n",
      "Test score: 0.24728829448699952\n",
      "Test accuracy: 0.9023199999618531\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "model.load_weights('Sentiment_Analysis_Amazon_Reviews.hdf5')\n",
    "#Test\n",
    "score, acc = model.evaluate(X_test, test_labels, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8516ee343c16c6025d733c7a7feb687118bfdbe"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "417e5115ccc79b85be7e6d697d0f1b61e8c8b412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset labels: [1, 0, 1]\n",
      "Predicted probability labels: [[0.9810409 ]\n",
      " [0.41099635]\n",
      " [0.98329055]]\n"
     ]
    }
   ],
   "source": [
    "cant = 3\n",
    "random_test = random.randint(1,len(X_test))\n",
    "X_test_cant = X_test[random_test:random_test+cant]\n",
    "#Prediction\n",
    "test_prediction = model.predict(X_test_cant)\n",
    "#test_prediction_labels = test_prediction.argmax(axis = -1)\n",
    "#Print predictions\n",
    "print(\"Dataset labels: {}\".format(test_labels[random_test:random_test+cant]))\n",
    "#print(\"Predicted labels: {}\".format(test_prediction_labels))\n",
    "print(\"Predicted probability labels: {}\".format(test_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3183665f345f23675687357705a3b96b749b5fa7",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

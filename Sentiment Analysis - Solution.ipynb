{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f",
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "%load_ext autoreload\n%autoreload 2",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8272ef6108d2803125980a8cbed4c5ae0be17c36"
      },
      "cell_type": "markdown",
      "source": "# Models"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a77de22781f477c899432c0a97324dbbc1b76e9b"
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input, SimpleRNN\nfrom keras.layers.embeddings import Embedding\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a88e005a021fb7902fbda4c8f0acff1ffa04664d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Input -> embed_size,maxlen,max_features,lr\n#Output -> model()\n\n#Multilayer - CNN - RNN model\ndef get_model_1(embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Dropout(0.25)(x)\n    x = Convolution1D(2*embed_size, kernel_size = 3)(x)\n    prefilt = Convolution1D(2*embed_size, kernel_size = 3)(x)\n    x = prefilt\n    for strides in [1, 1, 2]:\n        x = Convolution1D(128*2**(strides), strides = strides, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_size=3, kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n    x_f = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n    x_b = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n    x = concatenate([x_f, x_b])\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy'])\n    return model\n\n#MLP Model\ndef get_model_2 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Dense(128, activation=\"relu\")(x)    \n    x = Flatten()(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#CNN Model\ndef get_model_3 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)    \n    x = Flatten()(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#2 layers with Dropout - CNN Model\ndef get_model_4 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)   \n    x = Dropout(0.25)(x)\n    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)   \n    x = Dropout(0.25)(x)\n    x = Flatten()(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#3 layers with Dropout - CNN Model\ndef get_model_5 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)   \n    x = Dropout(0.25)(x)\n    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)   \n    x = Dropout(0.25)(x)\n    x = Convolution1D(embed_size, kernel_size = 3, activation = \"relu\")(x)   \n    x = Dropout(0.25)(x)\n    x = Flatten()(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#Simple RNN with Dropout\ndef get_model_6 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = SimpleRNN(256) (x)\n    x = Dropout(0.10)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#Simple LSTM with Dropout\ndef get_model_7 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = CuDNNLSTM(256) (x)\n    x = Dropout(0.10)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#Simple GRU with Dropout\ndef get_model_8 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = CuDNNGRU(256) (x)\n    x = Dropout(0.10)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#CNN-RNN model with Dropout\ndef get_model_9 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)   \n    x = Dropout(0.25)(x)\n    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)   \n    x = Dropout(0.10)(x)\n    x = CuDNNLSTM(256) (x)\n    x = Dropout(0.10)(x)    \n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#RNN-CNN model with Dropout\ndef get_model_10 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = CuDNNLSTM(256, return_sequences=True) (x)\n    x = Dropout(0.10)(x)   \n    x = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)   \n    x = Dropout(0.25)(x)\n    x = Convolution1D(2 * embed_size, kernel_size = 2, activation = \"relu\")(x)   \n    x = Dropout(0.10)(x)\n    x = Flatten()(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#CNN-RNN model with Concatenate and Dropout\ndef get_model_11 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Dropout(0.25)(x)\n    #Branch A\n    x_a = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)\n    x_a = Dropout(0.10)(x_a)\n    x_a = CuDNNLSTM(512)(x_a)  \n    #Branch B\n    x_b = Convolution1D(embed_size, kernel_size = 2, activation = \"relu\")(x)\n    x_b = Dropout(0.10)(x_b)\n    x_b = CuDNNLSTM(256, return_sequences=True)(x_b)  \n    x_b = Dropout(0.10)(x_b)\n    x_b = CuDNNLSTM(128)(x_b)  \n    #Concatenate Branch A-B\n    x = concatenate([x_a, x_b])\n    x = Dropout(0.25)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.25)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model\n\n#CNN-RNN-MLP model with Concatenate and Dropout\ndef get_model_12 (embed_size,maxlen,max_features,lr):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Dropout(0.25)(x)\n    #Branch A - 2 layers CNN + MLP\n    x_a = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)\n    x_a = Dropout(0.10)(x_a)\n    x_a = Convolution1D(2 * embed_size, kernel_size = 3, activation = \"relu\")(x)\n    x_a = Dropout(0.10)(x_a)\n    x_a = Dense(512, activation=\"relu\")(x_a)\n    x_a = Flatten()(x_a)\n    #Branch B - RNN + MLP\n    x_b = Convolution1D(embed_size, kernel_size = 2, activation = \"relu\")(x)\n    x_b = Dropout(0.10)(x_b)\n    x_b = CuDNNLSTM(256, return_sequences=True)(x_b)  \n    x_b = Dropout(0.10)(x_b)\n    x_b = Dense(512, activation=\"relu\")(x_b)\n    x_b = Flatten()(x_b)\n    #Concatenate Branch A-B\n    x = concatenate([x_a, x_b])\n    x = Dropout(0.25)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.25)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    ADAM = optimizers.Adam(lr=lr)\n    model.compile(loss = 'binary_crossentropy', optimizer=ADAM, metrics=['binary_accuracy'])\n    return model",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
      },
      "cell_type": "markdown",
      "source": "# Pre-processing"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "dba5fdee6eab073912fb38e690c776f48e0e4884"
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport chardet\nimport re\nimport os\nimport random",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f"
      },
      "cell_type": "code",
      "source": "#Checking files in Kaggle\n# List data files that are connected to the kernel\nos.listdir('../input')",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "['test.ft.txt.bz2', 'train.ft.txt.bz2']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Read Train & Test Files\n\n#Kaggle\ntrain_file = bz2.BZ2File('../input/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n\n#Localhost\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n\n#Localhost - VersiÃ³n recortada del archivo\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n\n#Create Lists containing Train & Test sentences\ntrain_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()\n\n#Convert from raw binary strings to strings that can be parsed\ntrain_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file, test_file\n#Garbage collector\ngc.collect()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2"
      },
      "cell_type": "code",
      "source": "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\nprint(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Cantidad de elementos del Training Set: 3600000\nCantidad de elementos del Testing Set: 400000\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
      },
      "cell_type": "markdown",
      "source": "## Clean data"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd"
      },
      "cell_type": "code",
      "source": "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n\n# Make everything Lower Case\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n\n# Modify URLs to <url>\nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
      },
      "cell_type": "markdown",
      "source": "## Checking data before and after cleaning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d"
      },
      "cell_type": "code",
      "source": "#Random\nr = random.randint(1,len(train_file_lines))\n\n#Before\nprint(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n\n#After\nprint(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n\n#Labels\nprint(\"\\nLabel:{}\".format(train_labels[r-1:r]))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Data before cleaning:\n['__label__2 Good shape but a a little worn: My book was in good shape but a little worn. Not quite the \"like new\" I expected but still worth the price.\\n']\n\nData after cleaning:\n['good shape but a a little worn: my book was in good shape but a little worn. not quite the \"like new\" i expected but still worth the price.']\n\nLabel:[1]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
      },
      "cell_type": "markdown",
      "source": "### Output\nFrom the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n\nSo, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file_lines, test_file_lines\n#Garbage collector\ngc.collect()",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
      },
      "cell_type": "markdown",
      "source": "## Text Pre-processing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing import text, sequence\n\n#Base definitions for text preprocessing\nmax_features = 20000\nmaxlen = 100",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff"
      },
      "cell_type": "code",
      "source": "#Tokenizer definition\n#Filtro caracteres especiales usando el Tokenizer de keras.\ntokenizer = text.Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n\n#Fit on text -> Only the train dataset !!!\ntokenizer.fit_on_texts(train_sentences)\n\n#Training set\ntokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n\n#Test set\ntokenized_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "becc7ba40bc713cec836c5ae061bc6553c4342dc"
      },
      "cell_type": "code",
      "source": "#Print a random matrix\nX_train[r]\n# summarize what was learned -> Si quiero ver el tokenizer que aprendio usando los 2 parametros (Max_features,max_length)\n#print(t.word_counts)\n#print(t.document_count)\n#print(t.word_index)\n#print(t.word_docs)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "array([ 1919,     2,   606,     1,  1211,   450,    22,     4,  2308,\n        3809, 19247,     2,  2763,   261,   152,    10,    79,   479,\n         158,    97,   111,    14,  4993,     5,    26,     4,   578,\n       12772, 16596,  9496,   335,   902,    79,   356,     2,  5251,\n           6,    17,    42,  3413,  1634,     2,   606,   169,     5,\n          50,   735,    12,    96,  5690,   824,     7,     1,  5690,\n          38, 14162,   102,   448,  2521,    37,    74,     1,   118,\n        1565,  2308, 16606,    61, 14565,  1220,     2,    42,   419,\n          38,   125,    37,    79,   921,  5298,     4,   215,   179,\n          50,    11,   207,  1347,    10,     1,  2308,  2143,  1018,\n           3,   395,    15,   279,     6,     9,    73,    22,     3,\n         101], dtype=int32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5381cf4aa759c3cde8c1a3bc775c71eaa0debfae"
      },
      "cell_type": "markdown",
      "source": "### Validation dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "034188709597cb9a4390840f8b4010b8e66a5b13",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n# Create a validation dataset\nvalidation_size = 0.2\nX_train, X_valid, train_labels, valid_labes = train_test_split(X_train, train_labels, test_size = validation_size)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9c36b66bfc3f7cfe53c50505956e443b643c319"
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences\n#Garbage collector\ngc.collect()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
      },
      "cell_type": "markdown",
      "source": "## Model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "666878741c1705787c1e9f6d3cc882ee3000b6c5"
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input\nfrom keras.layers.embeddings import Embedding",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f352c6d9b432671718da6f2d64083796924ebb28"
      },
      "cell_type": "code",
      "source": "#Defino los parametros del modelo:\nlr = 0.0001 #Learning Rate\nbatch_size = 1024\nepochs = 5\nembed_size = 128 #Embedding size",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3e60f578e04bea6df79525c9542e5dec03ac19d",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# <<<<Models>>>>\n#Input -> get_model_1(embed_size,maxlen,lr)\nmodel = get_model_2(embed_size,maxlen,max_features,lr)\nmodel.summary()",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_14 (InputLayer)        (None, 100)               0         \n_________________________________________________________________\nembedding_14 (Embedding)     (None, 100, 128)          2560000   \n_________________________________________________________________\ndense_21 (Dense)             (None, 100, 128)          16512     \n_________________________________________________________________\nflatten_8 (Flatten)          (None, 12800)             0         \n_________________________________________________________________\ndense_22 (Dense)             (None, 1)                 12801     \n=================================================================\nTotal params: 2,589,313\nTrainable params: 2,589,313\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a0611cfd88f87e629154694961c17ddf6e763002",
        "collapsed": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "## Callback para guardar pesos\ncheckpointer = ModelCheckpoint(filepath='Sentiment_Analysis_Amazon_Reviews.hdf5', monitor='val_loss'\n                                   ,verbose=1, save_best_only=True, mode='min')\ncallbacks = [checkpointer]",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "360004b4d48a606985a235a8f263b5457ab75bef",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "# Fit del modelo -> Usando todo el dataset\n\nmodel.fit(X_train,train_labels\n          ,epochs=epochs\n          ,batch_size = batch_size          \n          ,shuffle = True\n          ,validation_data = (X_valid,valid_labes)\n          ,callbacks=callbacks)\n\"\"\"\n# Fit del modelo -> Usando solo un fragmento del datasset\nmodel.fit(X_train[:100000], train_labels[:100000]\n          ,epochs=epochs\n          ,batch_size = batch_size          \n          ,shuffle = True\n          ,validation_split=0.20\n          ,callbacks=callbacks)\n\"\"\"",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 2880000 samples, validate on 720000 samples\nEpoch 1/5\n2880000/2880000 [==============================] - 69s 24us/step - loss: 0.3056 - binary_accuracy: 0.8665 - val_loss: 0.2541 - val_binary_accuracy: 0.8999\n\nEpoch 00001: val_loss improved from inf to 0.25410, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\nEpoch 2/5\n2880000/2880000 [==============================] - 65s 22us/step - loss: 0.2497 - binary_accuracy: 0.9012 - val_loss: 0.2514 - val_binary_accuracy: 0.9010\n\nEpoch 00002: val_loss improved from 0.25410 to 0.25144, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\nEpoch 3/5\n2880000/2880000 [==============================] - 65s 22us/step - loss: 0.2456 - binary_accuracy: 0.9032 - val_loss: 0.2488 - val_binary_accuracy: 0.9019\n\nEpoch 00003: val_loss improved from 0.25144 to 0.24885, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\nEpoch 4/5\n2880000/2880000 [==============================] - 65s 22us/step - loss: 0.2422 - binary_accuracy: 0.9048 - val_loss: 0.2473 - val_binary_accuracy: 0.9029\n\nEpoch 00004: val_loss improved from 0.24885 to 0.24728, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\nEpoch 5/5\n2880000/2880000 [==============================] - 65s 22us/step - loss: 0.2396 - binary_accuracy: 0.9059 - val_loss: 0.2466 - val_binary_accuracy: 0.9032\n\nEpoch 00005: val_loss improved from 0.24728 to 0.24664, saving model to Sentiment_Analysis_Amazon_Reviews.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "'\\n\\n# Fit del modelo -> Usando solo un fragmento del datasset\\nmodel.fit(X_train[:100000], train_labels[:100000]\\n          ,epochs=epochs\\n          ,batch_size = batch_size          \\n          ,shuffle = True\\n          ,validation_split=0.20\\n          ,callbacks=callbacks)\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "fd2138fd181f6d8302bc5d5697453bbdc8ff3f98"
      },
      "cell_type": "markdown",
      "source": "## Test "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64ea1632ab83398264b37480e87db3e4f5b60a34"
      },
      "cell_type": "code",
      "source": "#Load the model\nmodel.load_weights('Sentiment_Analysis_Amazon_Reviews.hdf5')\n#Test\nscore, acc = model.evaluate(X_test, test_labels, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "400000/400000 [==============================] - 3s 7us/step\nTest score: 0.24728829448699952\nTest accuracy: 0.9023199999618531\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b8516ee343c16c6025d733c7a7feb687118bfdbe"
      },
      "cell_type": "markdown",
      "source": "## Predict"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "417e5115ccc79b85be7e6d697d0f1b61e8c8b412"
      },
      "cell_type": "code",
      "source": "cant = 3\nrandom_test = random.randint(1,len(X_test))\nX_test_cant = X_test[random_test:random_test+cant]\n#Prediction\ntest_prediction = model.predict(X_test_cant)\n#test_prediction_labels = test_prediction.argmax(axis = -1)\n#Print predictions\nprint(\"Dataset labels: {}\".format(test_labels[random_test:random_test+cant]))\n#print(\"Predicted labels: {}\".format(test_prediction_labels))\nprint(\"Predicted probability labels: {}\".format(test_prediction))",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Dataset labels: [1, 0, 1]\nPredicted probability labels: [[0.9810409 ]\n [0.41099635]\n [0.98329055]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "3183665f345f23675687357705a3b96b749b5fa7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
{
  "cells": [
    {
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "_uuid": "433d791fa63fed8ca6c2e72ef3f706bd36f69e8f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%load_ext autoreload\n%autoreload 2",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8272ef6108d2803125980a8cbed4c5ae0be17c36"
      },
      "cell_type": "markdown",
      "source": "# Models"
    },
    {
      "metadata": {
        "_uuid": "a77de22781f477c899432c0a97324dbbc1b76e9b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, LSTM, GRU, CuDNNGRU, CuDNNLSTM, concatenate, Input, SimpleRNN\nfrom keras.layers.embeddings import Embedding\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b404ad0dcc33a9f313cf452b0bea553a1bb82f6"
      },
      "cell_type": "markdown",
      "source": "# Pre-processing"
    },
    {
      "metadata": {
        "_uuid": "dba5fdee6eab073912fb38e690c776f48e0e4884",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport chardet\nimport re\nimport os\nimport random",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ac33294d1ecf3b86e0db762f246acb73c0440f4f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Checking files in Kaggle\n# List data files that are connected to the kernel\n\n#os.listdir('../input')",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8baaebd44216dfdf4ff111acce0690760de706c0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Read Train & Test Files\n\n#Kaggle\ntrain_file = bz2.BZ2File('../input/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('../input/test.ft.txt.bz2')\n\n#Localhost\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/test.ft.txt.bz2')\n\n#Localhost - Versión recortada del archivo\n#train_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_train.ft.txt.bz2')\n#test_file = bz2.BZ2File('C:/Users/Lenovo/Documents/GitHub/Datasets/amazonreviews/Version_Recortada/r_test.ft.txt.bz2')\n\n#Create Lists containing Train & Test sentences\ntrain_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()\n\n#Convert from raw binary strings to strings that can be parsed\ntrain_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "22e5858babb6998f63d1f743bcff69559beea13e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file, test_file\n#Garbage collector\ngc.collect()",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "db3f94ee387a44b0827df0ca90081e7960e581b2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Cantidad de elementos del Training Set: {}\".format(len(train_file_lines)))\nprint(\"Cantidad de elementos del Testing Set: {}\".format(len(test_file_lines)))",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Cantidad de elementos del Training Set: 3600000\nCantidad de elementos del Testing Set: 400000\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e3d855e923c29dc8cfce89b35fd862a2b3aa5bba"
      },
      "cell_type": "markdown",
      "source": "## Clean data"
    },
    {
      "metadata": {
        "_uuid": "df31b59181d7e423fbb16d1405fed7ad62b1cbcd",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Change labels: __label__1 -> 0 (Negative) / __label__2 -> 1 (Positive)\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n\n# Make everything Lower Case\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n\n# Modify URLs to <url>\nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21719af8e34a93ecb7d9a97416f75b4a7e137a9a"
      },
      "cell_type": "markdown",
      "source": "## Checking data before and after cleaning"
    },
    {
      "metadata": {
        "_uuid": "8c03fbfe4735456cf3c0298c4747feba3424146d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Random\nr = random.randint(1,len(train_file_lines))\n\n#Before\nprint(\"Data before cleaning:\\n{}\".format(train_file_lines[r-1:r]))\n\n#After\nprint(\"\\nData after cleaning:\\n{}\".format((train_sentences[r-1:r])))\n\n#Labels\nprint(\"\\nLabel:{}\".format(train_labels[r-1:r]))",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Data before cleaning:\n[\"__label__1 pure dribble,all in dark,rotten audio: This is the biggest bunch of dribble I have ever seen. I think the whole film was shot in the dark. Even if meg ryan tryed to show something you would sure never see it. This has lousy audio, you can't hear snything and all of a sudden they try to break your eardrums with some car sound of something else dumb. I have no idea why meg ryan made this mess, If she needs money this bad mabe she should take up bartending\\n\"]\n\nData after cleaning:\n[\"pure dribble,all in dark,rotten audio: this is the biggest bunch of dribble i have ever seen. i think the whole film was shot in the dark. even if meg ryan tryed to show something you would sure never see it. this has lousy audio, you can't hear snything and all of a sudden they try to break your eardrums with some car sound of something else dumb. i have no idea why meg ryan made this mess, if she needs money this bad mabe she should take up bartending\"]\n\nLabel:[0]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c9a98cc678b3294d1ed9f86b36f3edc7e0cc690c"
      },
      "cell_type": "markdown",
      "source": "### Output\nFrom the above output it can be seen that each sentence begins with it's sentiment (label1 -> Negative, label2 -> Positive), which is then followed by the review and ends with a newline character \\n.\n\nSo, first I go convert all the labels to O(Negative) and 1(Positive) and store it in lists that only contain the label values. After this, I store the remainder of the sentence excluding the newline character in lowercase in lists. Also, convert all numbers to 0.\n"
    },
    {
      "metadata": {
        "_uuid": "96d7d285b63ffa8ddf0a70e8df2f4d765b8073e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Delete memory reference (?)\ndel train_file_lines, test_file_lines\n#Garbage collector\ngc.collect()",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "28e76653368b7b61fecaacdd4bdd3c10b256ead6"
      },
      "cell_type": "markdown",
      "source": "### Using only a Percentage of train set"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30dee854c109ed19ee46064abb1a803630f078af"
      },
      "cell_type": "code",
      "source": "# Esto lo hago porque tarda mucho tiempo en entrenar => Me quedo con un % mas pequeño del dataset.\nflag_size = 1\nfrom sklearn.model_selection import train_test_split\nif flag_size == 1:\n    # Create a validation dataset\n    size = 0.8 #Dejo el X para lo que sería valid => Me queda el (1-X) que voy a utilizar.\n    train_sentences, valid_sentences, train_labels, valid_labes = train_test_split(train_sentences, train_labels, test_size = size)\n    \nprint(len(train_sentences))\nprint(len(valid_sentences))",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "720000\n2880000\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4bf6a4ae2b9cdb72178ccb1afb63795c88efb866"
      },
      "cell_type": "markdown",
      "source": "## Text Pre-processing"
    },
    {
      "metadata": {
        "_uuid": "cac4d50ebe788b552a7c85fd21951f1cbe513533",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer\n\n#Delete special characters -> In Keras I use the Filter.\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub(\"[^a-zA-Z]\", \" \",train_sentences[i])\n    \nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub(\"[^a-zA-Z]\", \" \",train_sentences[i])\n    \n#Base definitions for text preprocessing\nmax_features = 20000\nmaxlen = 100",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "78fdef3b5107224fecba3cc4720eb693b2f048ff",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n\nv = CountVectorizer(analyzer = \"word\",max_features = max_features)\n\nX_train = v.fit_transform(train_sentences)\nX_test = v.transform(test_sentences)",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4442dfc514e5e9228aab0fec45e04d55a400b35d"
      },
      "cell_type": "markdown",
      "source": "## Model"
    },
    {
      "metadata": {
        "_uuid": "9ba4e755b369728f6b3ec6007c5bbb117ba098f9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\n\ntrain_labels_array = np.array(train_labels)\ntest_labels_array = np.array(test_labels)",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e9e3ddb344620e6b78d6356dde6dddc50b3c861d"
      },
      "cell_type": "markdown",
      "source": "### Logistic Regression"
    },
    {
      "metadata": {
        "_uuid": "c1e6c3d5a86ee18206ac22f67ad2641962269ac6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "classifier = LogisticRegression(max_iter=200)\n\nfit = classifier.fit(X_train,train_labels_array)\npred = fit.predict(X_test)\naccuracy = accuracy_score(pred,test_labels_array)\n\nprint('Accuracy of Logistic Regression is '+str(accuracy))    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "849e549b918088dc0cebba9fa684d67d61199ad1"
      },
      "cell_type": "markdown",
      "source": "### Linear SVC"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a940e19ac9b654460aba50558e5d6ba830c8c316"
      },
      "cell_type": "code",
      "source": "classifier = LinearSVC(max_iter=200)\n\nfit = classifier.fit(X_train,train_labels_array)\npred = fit.predict(X_test)\naccuracy = accuracy_score(pred,test_labels_array)\n\nprint('Accuracy of Linear SVC is '+str(accuracy)) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "84e1ad064060085c35324f97ce7ac0a4ef7fce22"
      },
      "cell_type": "markdown",
      "source": "### NuSVC"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6faf1f8624a3955f25a380f38f3ce86d5bccbe0"
      },
      "cell_type": "code",
      "source": "classifier = NuSVC(max_iter=200)\n\nfit = classifier.fit(X_train,train_labels_array)\npred = fit.predict(X_test)\naccuracy = accuracy_score(pred,test_labels_array)\n\nprint('Accuracy of NuSVC is '+str(accuracy)) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ebf61a4d11594211d16f546cdaf3621eaf965ba"
      },
      "cell_type": "markdown",
      "source": "### Random Forest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73fb7a3d6ad36f52dbcb58b28b2468dc1f38eb0d"
      },
      "cell_type": "code",
      "source": "classifier = RandomForestClassifier(n_estimators=200)\n\nfit = classifier.fit(X_train,train_labels_array)\npred = fit.predict(X_test)\naccuracy = accuracy_score(pred,test_labels_array)\n\nprint('Accuracy of Random Forest Classifier is '+str(accuracy)) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ccf12c36f300120042d721b383c1d278afb991bc"
      },
      "cell_type": "markdown",
      "source": "### AdaBoostClassifier"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "170fcbf732af2659f4471d7c1ae7d165ffe59260"
      },
      "cell_type": "code",
      "source": "classifier = AdaBoostClassifier()\n\nfit = classifier.fit(X_train,train_labels_array)\npred = fit.predict(X_test)\naccuracy = accuracy_score(pred,test_labels_array)\n\nprint('Accuracy ofAdaBoostClassifier is '+str(accuracy)) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2a5a32873d71a493b4b06f4d1594a8063866ebb7"
      },
      "cell_type": "markdown",
      "source": "### KNeighborsClassifier"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb2ed3d81639299454fa6d0812350cd78228863e"
      },
      "cell_type": "code",
      "source": "classifier = KNeighborsClassifier(2) #Son solo 2 grupos\n\nfit = classifier.fit(X_train,train_labels_array)\npred = fit.predict(X_test)\naccuracy = accuracy_score(pred,test_labels_array)\n\nprint('Accuracy of KNeighborsClassifier is '+str(accuracy)) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc315e942282741154a8451a0bdd5948c0d8aec9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}